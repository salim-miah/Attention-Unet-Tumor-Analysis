{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":13019382,"sourceType":"datasetVersion","datasetId":8237965},{"sourceId":261069468,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport random\nfrom typing import Optional, Tuple, Dict, Any\n\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom google.colab import drive","metadata":{"id":"PsqNqGX58Exh","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:37:43.623624Z","iopub.execute_input":"2025-09-11T03:37:43.625547Z","iopub.status.idle":"2025-09-11T03:37:50.074797Z","shell.execute_reply.started":"2025-09-11T03:37:43.625480Z","shell.execute_reply":"2025-09-11T03:37:50.074251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_path = \"/kaggle/input/brisc-dataset/train_index1.csv\"  # user-provided path\nimage_col = \"image_path\"\nmask_col = \"mask_path\"\nclass_col = \"cls_label\"","metadata":{"id":"BnTh7pqA8V5e","outputId":"1f8815ce-1c9d-43bc-ead6-f2ba2f944685","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:37:55.943364Z","iopub.execute_input":"2025-09-11T03:37:55.943631Z","iopub.status.idle":"2025-09-11T03:37:55.947377Z","shell.execute_reply.started":"2025-09-11T03:37:55.943611Z","shell.execute_reply":"2025-09-11T03:37:55.946674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set according to dataset\nseg_num_classes = 2          # e.g., background=0, tumor=1\nclf_num_classes = 4          # from CSV, appears to have labels like 1,3,... adjust to actual class count\ninput_size = (256, 256)      # training crop/resize\nbatch_size = 8\nnum_workers = 2\nseed = 42\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Loss weights for joint training\nlambda_seg = 1.0\nlambda_clf = 1.0","metadata":{"id":"bmVkdCqn9Tc2","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:38:09.786942Z","iopub.execute_input":"2025-09-11T03:38:09.787462Z","iopub.status.idle":"2025-09-11T03:38:09.856520Z","shell.execute_reply.started":"2025-09-11T03:38:09.787439Z","shell.execute_reply":"2025-09-11T03:38:09.855715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(s=42):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n\nset_seed(seed)\n\ndef str_or_nan(s):\n    if pd.isna(s) or (isinstance(s, float) and math.isnan(s)):\n        return None\n    s = str(s).strip()\n    return s if s else None\n","metadata":{"id":"xQCKqf4l9uSc","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:38:25.079275Z","iopub.execute_input":"2025-09-11T03:38:25.079800Z","iopub.status.idle":"2025-09-11T03:38:25.089187Z","shell.execute_reply.started":"2025-09-11T03:38:25.079778Z","shell.execute_reply":"2025-09-11T03:38:25.088579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegClfDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transforms: Optional[A.BasicTransform]=None, seg_num_classes: int=2, input_size: Tuple[int, int]=(512, 512)):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n        self.seg_num_classes = seg_num_classes\n        self.input_size = input_size\n\n\n        # Pre-parse items\n        self.items = []\n        for _, row in self.df.iterrows():\n            img_path = str(row[image_col]).strip()\n            mask_path = str_or_nan(row[mask_col])\n            cls_label = int(row[class_col])\n\n            # Some rows list a mask that may not exist on disk; treat as missing if file not found\n            has_mask = mask_path is not None and os.path.exists(mask_path)\n            self.items.append({\n                \"img\": img_path,\n                \"mask\": mask_path if has_mask else None,\n                \"has_mask\": has_mask,\n                \"cls\": cls_label\n            })\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item = self.items[idx]\n        img = np.array(Image.open(item[\"img\"]).convert(\"RGB\"))\n\n        mask = None\n        if item[\"has_mask\"]:\n            # Expecting integer labels in mask\n            mask_pil = Image.open(item[\"mask\"])\n            mask = np.array(mask_pil)\n            # If mask is RGB, convert to single channel by taking one channel or remapping as needed\n            if mask.ndim == 3:\n                # Heuristic: if mask is RGB with color coding, take one channel (assuming binary), adapt if needed\n                mask = mask[..., 0]\n            # Ensure mask contains values 0..(seg_num_classes-1)\n            # If masks are {0,255}, map 255->1 for binary\n            if self.seg_num_classes == 2 and mask.max() > 1:\n                mask = (mask > 0).astype(np.uint8)\n\n        if self.transforms is not None:\n            if mask is not None:\n                aug = self.transforms(image=img, mask=mask)\n                img = aug[\"image\"]\n                mask = aug[\"mask\"]\n            else:\n                aug = self.transforms(image=img)\n                img = aug[\"image\"]\n                mask = None\n\n        sample = {\n            \"image\": img,     # Tensor CxHxW\n            \"label\": torch.tensor(item[\"cls\"], dtype=torch.long)\n        }\n        if mask is not None:\n            # For CrossEntropyLoss, mask should be Long with shape HxW\n            if isinstance(mask, torch.Tensor):\n                # Already tensor via ToTensorV2\n                mask_t = mask.long().squeeze()\n            else:\n                mask_t = torch.from_numpy(mask).long()\n            sample[\"mask\"] = mask_t\n            sample[\"has_mask\"] = torch.tensor(1, dtype=torch.uint8)\n        else:\n            # Create a zero mask of the expected size\n            sample[\"mask\"] = torch.zeros(self.input_size, dtype=torch.long)\n            sample[\"has_mask\"] = torch.tensor(0, dtype=torch.uint8)\n\n\n        return sample","metadata":{"id":"QNjl4j_N95ic","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:38:29.582287Z","iopub.execute_input":"2025-09-11T03:38:29.582522Z","iopub.status.idle":"2025-09-11T03:38:29.592361Z","shell.execute_reply.started":"2025-09-11T03:38:29.582507Z","shell.execute_reply":"2025-09-11T03:38:29.591496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tf = A.Compose([\n    A.LongestMaxSize(max(input_size)),\n    A.PadIfNeeded(min_height=512, min_width=512, border_mode=0, value=0, mask_value=0),\n    A.RandomCrop(height=512, width=512),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    # Elastic deformation is a key augmentation in U-Net paper\n    A.ElasticTransform(p=0.3, alpha=50, sigma=6, alpha_affine=20),\n    A.ColorJitter(p=0.3, brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\nval_tf = A.Compose([\n    A.LongestMaxSize(max(input_size)),\n    A.PadIfNeeded(min_height=512, min_width=512, border_mode=0, value=0, mask_value=0),\n    A.CenterCrop(height=512, width=512),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])","metadata":{"id":"8qMCThD297Ip","outputId":"b544cddd-6cb6-44fd-baa5-ee038ca00c92","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:38:35.073817Z","iopub.execute_input":"2025-09-11T03:38:35.074523Z","iopub.status.idle":"2025-09-11T03:38:35.088435Z","shell.execute_reply.started":"2025-09-11T03:38:35.074497Z","shell.execute_reply":"2025-09-11T03:38:35.087782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_ch, out_ch)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super().__init__()\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n            self.conv = DoubleConv(in_ch, out_ch)\n        else:\n            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # Pad x1 to the size of x2 if needed\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3] # Access dimensions directly\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX//2, diffY // 2, diffY - diffY//2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\nclass UNetWithClassifier(nn.Module):\n    def __init__(self, n_channels=3, n_classes=2, clf_classes=4, bilinear=True, base_ch=64):\n        super().__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        # Encoder\n        self.inc = DoubleConv(n_channels, base_ch)         # 64\n        self.down1 = Down(base_ch, base_ch*2)              # 128\n        self.down2 = Down(base_ch*2, base_ch*4)            # 256\n        self.down3 = Down(base_ch*4, base_ch*8)            # 512\n        factor = 2 if bilinear else 1\n        self.down4 = Down(base_ch*8, base_ch*16 // factor) # 1024->512 if bilinear\n\n        # Decoder (Segmentation head)\n        self.up1 = Up(base_ch*16, base_ch*8 // factor, bilinear=bilinear)\n        self.up2 = Up(base_ch*8, base_ch*4 // factor, bilinear=bilinear)\n        self.up3 = Up(base_ch*4, base_ch*2 // factor, bilinear=bilinear)\n        self.up4 = Up(base_ch*2, base_ch, bilinear=bilinear)\n        self.seg_out = nn.Conv2d(base_ch, n_classes, kernel_size=1)\n\n        # Classifier head on bottleneck\n        bottleneck_ch = base_ch*16 // factor\n        self.clf_pool = nn.AdaptiveAvgPool2d(1)\n        self.clf_head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(bottleneck_ch, bottleneck_ch//2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(bottleneck_ch//2, clf_classes)\n        )\n\n    def encode(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        return x1, x2, x3, x4, x5\n\n    def decode(self, x1, x2, x3, x4, x5):\n        y = self.up1(x5, x4)\n        y = self.up2(y, x3)\n        y = self.up3(y, x2)\n        y = self.up4(y, x1)\n        logits = self.seg_out(y)\n        return logits\n\n    def classify(self, x5):\n        pooled = self.clf_pool(x5)\n        logits = self.clf_head(pooled)\n        return logits\n\n    def forward(self, x, return_both=True):\n        x1, x2, x3, x4, x5 = self.encode(x)\n        seg_logits = self.decode(x1, x2, x3, x4, x5) if return_both else None\n        clf_logits = self.classify(x5)\n        return seg_logits, clf_logits","metadata":{"id":"kuJQIio5A5UD","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:38:39.714470Z","iopub.execute_input":"2025-09-11T03:38:39.714753Z","iopub.status.idle":"2025-09-11T03:38:39.730356Z","shell.execute_reply.started":"2025-09-11T03:38:39.714730Z","shell.execute_reply":"2025-09-11T03:38:39.729617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Losses and metrics\n# -----------------------------\ndef dice_coefficient(pred_logits, target, num_classes, eps=1e-6):\n    # pred: NxCxHxW logits -> softmax to probs\n    probs = F.softmax(pred_logits, dim=1)\n    # One-hot target\n    n, c, h, w = probs.shape\n    target_oh = F.one_hot(target, num_classes=c).permute(0,3,1,2).float()\n    dims = (0,2,3)\n    intersection = torch.sum(probs * target_oh, dims)\n    cardinality = torch.sum(probs + target_oh, dims)\n    dice = (2. * intersection + eps) / (cardinality + eps)\n    # Return mean over classes\n    return dice.mean()","metadata":{"id":"d2CTOCf8A7Vt","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:38:51.101897Z","iopub.execute_input":"2025-09-11T03:38:51.102483Z","iopub.status.idle":"2025-09-11T03:38:51.107047Z","shell.execute_reply.started":"2025-09-11T03:38:51.102458Z","shell.execute_reply":"2025-09-11T03:38:51.106362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Training helpers\n# -----------------------------\ndef train_one_epoch(model, loader, optimizer, seg_criterion, clf_criterion, mode=\"joint\"):\n    model.train()\n    total_loss = 0.0\n    total_seg_loss = 0.0\n    total_clf_loss = 0.0\n    total = 0\n\n    for batch in loader:\n        imgs = batch[\"image\"].to(device)\n        labels = batch[\"label\"].to(device)\n        has_mask = batch[\"has_mask\"].to(device).float()\n        masks = batch[\"mask\"].to(device)\n\n        optimizer.zero_grad()\n\n        if mode == \"seg_only\":\n            seg_logits, _ = model(imgs, return_both=True)\n            # Compute loss for all samples and mask out those without masks\n            seg_loss = seg_criterion(seg_logits, masks)\n            # Ensure has_mask is broadcastable for element-wise multiplication\n            seg_loss = (seg_loss * has_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3)).mean()\n            loss = seg_loss\n            clf_loss = torch.tensor(0.0, device=device)\n\n        elif mode == \"clf_only\":\n            _, clf_logits = model(imgs, return_both=False)  # skip decoder computation\n            clf_loss = clf_criterion(clf_logits, labels)\n            loss = clf_loss\n            seg_loss = torch.tensor(0.0, device=device)\n\n        else:  # joint\n            seg_logits, clf_logits = model(imgs, return_both=True)\n            # Segmentation loss on available masks\n            seg_loss = seg_criterion(seg_logits, masks)\n            # Ensure has_mask is broadcastable for element-wise multiplication\n            seg_loss = (seg_loss * has_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3)).mean()\n\n            clf_loss = clf_criterion(clf_logits, labels)\n            loss = lambda_seg * seg_loss + lambda_clf * clf_loss\n\n        loss.backward()\n        optimizer.step()\n\n        bs = imgs.size(0)\n        total += bs\n        total_loss += loss.item() * bs\n        total_seg_loss += seg_loss.item() * bs\n        total_clf_loss += clf_loss.item() * bs\n\n    return {\n        \"loss\": total_loss / total,\n        \"seg_loss\": total_seg_loss / total,\n        \"clf_loss\": total_clf_loss / total\n    }\n\n@torch.no_grad()\ndef evaluate(model, loader, seg_criterion, clf_criterion, mode=\"joint\"):\n    model.eval()\n    total_loss = 0.0\n    total_seg_loss = 0.0\n    total_clf_loss = 0.0\n    total = 0\n    # Metrics\n    total_dice = 0.0\n    dice_count = 0\n    correct = 0\n    total_samples = 0\n\n    # Initialize seg_loss_masked\n    seg_loss_masked = torch.tensor(0.0, device=device)\n\n    for batch in loader:\n        imgs = batch[\"image\"].to(device)\n        labels = batch[\"label\"].to(device)\n        has_mask = batch[\"has_mask\"].to(device).float()\n        masks = batch[\"mask\"].to(device)\n\n        if mode == \"seg_only\":\n            seg_logits, _ = model(imgs, return_both=True)\n            seg_loss = seg_criterion(seg_logits, masks)\n            # Ensure has_mask is broadcastable for element-wise multiplication\n            seg_loss_masked = (seg_loss * has_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3)).mean()\n\n            if has_mask.sum() > 0:\n                # Dice on masked samples (only for samples with masks)\n                # Need to filter seg_logits and masks based on has_mask\n                seg_logits_masked = seg_logits[has_mask.bool()]\n                masks_masked = masks[has_mask.bool()]\n                if seg_logits_masked.numel() > 0 and masks_masked.numel() > 0:\n                    total_dice += dice_coefficient(seg_logits_masked, masks_masked, num_classes=model.n_classes).item()\n                    dice_count += 1\n\n            loss = seg_loss_masked\n            clf_loss = torch.tensor(0.0, device=device)\n\n        elif mode == \"clf_only\":\n            _, clf_logits = model(imgs, return_both=False)\n            clf_loss = clf_criterion(clf_logits, labels)\n            loss = clf_loss\n            seg_loss = torch.tensor(0.0, device=device)\n\n            # Accuracy\n            preds = clf_logits.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n\n        else:  # joint\n            seg_logits, clf_logits = model(imgs, return_both=True)\n            seg_loss = seg_criterion(seg_logits, masks)\n            # Ensure has_mask is broadcastable for element-wise multiplication\n            seg_loss_masked = (seg_loss * has_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3)).mean()\n\n            if has_mask.sum() > 0:\n                 # Dice on masked samples (only for samples with masks)\n                seg_logits_masked = seg_logits[has_mask.bool()]\n                masks_masked = masks[has_mask.bool()]\n                if seg_logits_masked.numel() > 0 and masks_masked.numel() > 0:\n                    total_dice += dice_coefficient(seg_logits_masked, masks_masked, num_classes=model.n_classes).item()\n                    dice_count += 1\n\n            clf_loss = clf_criterion(clf_logits, labels)\n            loss = lambda_seg * seg_loss_masked + lambda_clf * clf_loss\n\n            preds = clf_logits.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n\n        bs = imgs.size(0)\n        total += bs\n        total_loss += loss.item() * bs\n        total_seg_loss += seg_loss_masked.item() * bs # Use masked loss for logging\n        total_clf_loss += clf_loss.item() * bs\n\n    metrics = {\n        \"loss\": total_loss / total,\n        \"seg_loss\": total_seg_loss / total,\n        \"clf_loss\": total_clf_loss / total,\n    }\n    if dice_count > 0:\n        metrics[\"dice\"] = total_dice / dice_count\n    if total_samples > 0:\n        metrics[\"acc\"] = correct / total_samples\n    return metrics","metadata":{"id":"0Id72RBaBEn4","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:38:55.184107Z","iopub.execute_input":"2025-09-11T03:38:55.184659Z","iopub.status.idle":"2025-09-11T03:38:55.198822Z","shell.execute_reply.started":"2025-09-11T03:38:55.184636Z","shell.execute_reply":"2025-09-11T03:38:55.197926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Data split and loaders\n# -----------------------------\ndf = pd.read_csv(csv_path)\n# Basic sanity: ensure files exist; drop missing images\ndf = df[df[image_col].apply(lambda p: isinstance(p, str) and os.path.exists(p))].reset_index(drop=True)\n\n# Stratify by class for balanced splits\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=seed, stratify=df[class_col])\n\ntrain_ds = SegClfDataset(train_df, transforms=train_tf, seg_num_classes=seg_num_classes)\nval_ds   = SegClfDataset(val_df,   transforms=val_tf,   seg_num_classes=seg_num_classes)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\nval_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","metadata":{"id":"VTkBJh6WBQkB","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:39:01.783818Z","iopub.execute_input":"2025-09-11T03:39:01.784091Z","iopub.status.idle":"2025-09-11T03:39:26.041767Z","shell.execute_reply.started":"2025-09-11T03:39:01.784070Z","shell.execute_reply":"2025-09-11T03:39:26.040838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Instantiate model and criteria\n# -----------------------------\nmodel = UNetWithClassifier(n_channels=3, n_classes=seg_num_classes, clf_classes=clf_num_classes, bilinear=True, base_ch=64).to(device)\n\n# Losses\nseg_criterion = nn.CrossEntropyLoss()  \nclf_criterion = nn.CrossEntropyLoss()","metadata":{"id":"eHrlHz4dBWCv","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:39:38.577933Z","iopub.execute_input":"2025-09-11T03:39:38.578622Z","iopub.status.idle":"2025-09-11T03:39:38.912924Z","shell.execute_reply.started":"2025-09-11T03:39:38.578594Z","shell.execute_reply":"2025-09-11T03:39:38.912338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Optimizers and schedulers\n# -----------------------------\ndef make_optimizer(lr=1e-3, wd=1e-4):\n    return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n\ndef make_scheduler(optimizer):\n    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)","metadata":{"id":"bAUxJAoXBdLZ","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:39:42.502633Z","iopub.execute_input":"2025-09-11T03:39:42.503232Z","iopub.status.idle":"2025-09-11T03:39:42.507231Z","shell.execute_reply.started":"2025-09-11T03:39:42.503212Z","shell.execute_reply":"2025-09-11T03:39:42.506331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Training loops\n# -----------------------------\n\n# =============================\n# Logging, plotting, checkpoints\n# =============================\nimport os, json, torch\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nout_dir = \"/kaggle/working/\"\nckpt_dir = os.path.join(out_dir, \"checkpoints\")\nfig_dir = os.path.join(out_dir, \"figs\")\nos.makedirs(ckpt_dir, exist_ok=True)\nos.makedirs(fig_dir, exist_ok=True)\n\ndef run_training_with_logging(mode=\"seg_only\", epochs=10, lr=1e-3, tag=None):\n    assert mode in [\"seg_only\", \"clf_only\", \"joint\"]\n    optimizer = make_optimizer(lr=lr)\n    scheduler = make_scheduler(optimizer)\n\n    best_val = float('inf')\n    best_state = None\n\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"train_seg_loss\": [],\n        \"train_clf_loss\": [],\n        \"val_loss\": [],\n        \"val_seg_loss\": [],\n        \"val_clf_loss\": [],\n        \"val_dice\": [],\n        \"val_acc\": []\n    }\n\n    time_tag = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    run_tag = tag or f\"{mode}_{time_tag}\"\n\n    for epoch in range(1, epochs+1):\n        tr_metrics = train_one_epoch(model, train_loader, optimizer, seg_criterion, clf_criterion, mode=mode)\n        val_metrics = evaluate(model, val_loader, seg_criterion, clf_criterion, mode=mode)\n\n        scheduler.step(val_metrics[\"loss\"])\n\n        # Save per-epoch checkpoint (full state)\n        torch.save({\n            \"epoch\": epoch,\n            \"mode\": mode,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"val_loss\": val_metrics[\"loss\"],\n        }, os.path.join(ckpt_dir, f\"{run_tag}_epoch{epoch:02d}.pth\"))\n\n        # Track best weights\n        if val_metrics[\"loss\"] < best_val:\n            best_val = val_metrics[\"loss\"]\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n            torch.save(best_state, os.path.join(ckpt_dir, f\"{run_tag}_best_weights.pth\"))\n\n        # Log history\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(tr_metrics[\"loss\"])\n        history[\"train_seg_loss\"].append(tr_metrics.get(\"seg_loss\", 0.0))\n        history[\"train_clf_loss\"].append(tr_metrics.get(\"clf_loss\", 0.0))\n        history[\"val_loss\"].append(val_metrics[\"loss\"])\n        history[\"val_seg_loss\"].append(val_metrics.get(\"seg_loss\", 0.0))\n        history[\"val_clf_loss\"].append(val_metrics.get(\"clf_loss\", 0.0))\n        history[\"val_dice\"].append(val_metrics.get(\"dice\", None))\n        history[\"val_acc\"].append(val_metrics.get(\"acc\", None))\n\n        print(f\"[{mode}] Epoch {epoch:02d}: \"\n              f\"train_loss={tr_metrics['loss']:.4f} seg={tr_metrics['seg_loss']:.4f} clf={tr_metrics['clf_loss']:.4f} | \"\n              f\"val_loss={val_metrics['loss']:.4f} seg={val_metrics['seg_loss']:.4f} clf={val_metrics['clf_loss']:.4f}\")\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # Save history JSON\n    with open(os.path.join(out_dir, f\"history_{run_tag}.json\"), \"w\") as f:\n        json.dump(history, f, indent=2)\n\n    # Plot helpers\n    def _plot_series(xs, series, title, fname, ylabel=\"Loss\"):\n        plt.figure(figsize=(7,5))\n        for lbl, ys in series.items():\n            if ys is None: \n                continue\n            plt.plot(xs, ys, label=lbl)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(ylabel)\n        plt.title(title)\n        plt.grid(True, alpha=0.3)\n        plt.legend()\n        plt.tight_layout()\n        path = os.path.join(fig_dir, fname)\n        plt.savefig(path, dpi=150)\n        plt.close()\n        return path\n\n    epochs_list = history[\"epoch\"]\n    if mode == \"seg_only\":\n        _plot_series(\n            epochs_list,\n            {\"train_seg_loss\": history[\"train_seg_loss\"], \"val_seg_loss\": history[\"val_seg_loss\"]},\n            \"Segmentation Loss (seg_only)\",\n            f\"{run_tag}_seg_losses.png\"\n        )\n    elif mode == \"clf_only\":\n        _plot_series(\n            epochs_list,\n            {\"train_clf_loss\": history[\"train_clf_loss\"], \"val_clf_loss\": history[\"val_clf_loss\"]},\n            \"Classification Loss (clf_only)\",\n            f\"{run_tag}_clf_losses.png\"\n        )\n    else:  # joint\n        _plot_series(\n            epochs_list,\n            {\n                \"train_seg_loss\": history[\"train_seg_loss\"],\n                \"val_seg_loss\": history[\"val_seg_loss\"],\n                \"train_clf_loss\": history[\"train_clf_loss\"],\n                \"val_clf_loss\": history[\"val_clf_loss\"]\n            },\n            \"Joint Training Losses (Seg + Clf)\",\n            f\"{run_tag}_joint_losses.png\"\n        )\n        # Optional: plot total losses too\n        _plot_series(\n            epochs_list,\n            {\"train_total\": history[\"train_loss\"], \"val_total\": history[\"val_loss\"]},\n            \"Total Loss (Joint)\",\n            f\"{run_tag}_total_losses.png\"\n        )\n\n    return best_val, history\n\n# Examples: short runs to generate figures and checkpoints quickly (adjust epochs as needed)\n# best_val_seg, hist_seg = run_training_with_logging(mode=\"seg_only\", epochs=10, lr=1e-3, tag=\"seg_only_main\")\n# best_val_clf, hist_clf = run_training_with_logging(mode=\"clf_only\", epochs=10, lr=1e-3, tag=\"clf_only_main\")\n# best_val_joint, hist_joint = run_training_with_logging(mode=\"joint\", epochs=15, lr=1e-3, tag=\"joint_main\")\n\n\n# def run_training(mode=\"seg_only\", epochs=10, lr=1e-3):\n#     assert mode in [\"seg_only\", \"clf_only\", \"joint\"]\n#     optimizer = make_optimizer(lr=lr)\n#     scheduler = make_scheduler(optimizer)\n\n#     best_val = float('inf')\n#     best_state = None\n\n#     for epoch in range(1, epochs+1):\n#         tr_metrics = train_one_epoch(model, train_loader, optimizer, seg_criterion, clf_criterion, mode=mode)\n#         val_metrics = evaluate(model, val_loader, seg_criterion, clf_criterion, mode=mode)\n\n#         # Choose scheduler metric based on mode\n#         sched_metric = val_metrics[\"loss\"]\n#         scheduler.step(sched_metric)\n\n#         # Simple model selection by val loss\n#         if val_metrics[\"loss\"] < best_val:\n#             best_val = val_metrics[\"loss\"]\n#             best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n\n#         # Log\n#         msg = f\"[{mode}] Epoch {epoch:02d}: \"\n#         msg += f\"train_loss={tr_metrics['loss']:.4f} \"\n#         msg += f\"val_loss={val_metrics['loss']:.4f} \"\n#         if \"dice\" in val_metrics:\n#             msg += f\"val_dice={val_metrics['dice']:.4f} \"\n#         if \"acc\" in val_metrics:\n#             msg += f\"val_acc={val_metrics['acc']:.4f} \"\n#         print(msg)\n\n#     # Load best\n#     if best_state is not None:\n#         model.load_state_dict(best_state)\n#     return best_val","metadata":{"id":"SeSgGaiWBePf","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:39:49.505388Z","iopub.execute_input":"2025-09-11T03:39:49.505693Z","iopub.status.idle":"2025-09-11T03:39:49.520762Z","shell.execute_reply.started":"2025-09-11T03:39:49.505672Z","shell.execute_reply":"2025-09-11T03:39:49.520246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Experiments: separate vs joint\n# -----------------------------\n# 1) Train segmentation-only (decoder head) – classification head is not optimized if using mode=\"seg_only\"\nprint(\"Training segmentation-only...\")\nbest_val_seg, hist_seg = run_training_with_logging(mode=\"seg_only\", epochs=10, lr=1e-3, tag=\"seg_only_main\")","metadata":{"id":"iZMTgtoIBmB9","outputId":"f869bdf5-685b-49ff-d8ca-8a7fdd0ace00","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2) Train classification-only (classifier head) – decoder not used in forward if mode=\\\"clf_only\\\"\nprint(\"Training classification-only...\")\nbest_val_clf, hist_clf = run_training_with_logging(mode=\"clf_only\", epochs=10, lr=1e-3, tag=\"clf_only_main\")","metadata":{"id":"LMN-8fjhD78G","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3) Train jointly – end-to-end multi-task\nprint(\"Training joint multi-task...\")\nbest_val_joint, hist_joint = run_training_with_logging(mode=\"joint\", epochs=15, lr=1e-3, tag=\"joint_main\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Checkpoints ->\", ckpt_dir)\nprint(\"Figures     ->\", fig_dir)\nprint(\"Histories   ->\", out_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AttentionBlock(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        \n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        \n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        \n        self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self, g, x):\n        # Upsample gating signal to match spatial dimensions of x\n        g1 = self.W_g(g)\n        if g1.size()[2:] != x.size()[2:]:\n            g1 = F.interpolate(g1, size=x.size()[2:], mode='bilinear', align_corners=True)\n        \n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        \n        return x * psi\n\n\nclass AttentionUNetWithClassifier(nn.Module):\n    def __init__(self, n_channels=3, n_classes=2, clf_classes=4, bilinear=True, base_ch=64):\n        super().__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n        \n        # Encoder (same as regular U-Net)\n        self.inc = DoubleConv(n_channels, base_ch)         # 64\n        self.down1 = Down(base_ch, base_ch*2)              # 128\n        self.down2 = Down(base_ch*2, base_ch*4)            # 256\n        self.down3 = Down(base_ch*4, base_ch*8)            # 512\n        factor = 2 if bilinear else 1\n        self.down4 = Down(base_ch*8, base_ch*16 // factor) # 1024->512 if bilinear\n        \n        # Attention gates\n        self.attention1 = AttentionBlock(F_g=base_ch*16//factor, F_l=base_ch*8, F_int=base_ch*4)\n        self.attention2 = AttentionBlock(F_g=base_ch*8//factor, F_l=base_ch*4, F_int=base_ch*2)\n        self.attention3 = AttentionBlock(F_g=base_ch*4//factor, F_l=base_ch*2, F_int=base_ch)\n        \n        # Decoder with attention\n        self.up1 = Up(base_ch*16, base_ch*8 // factor, bilinear=bilinear)\n        self.up2 = Up(base_ch*8, base_ch*4 // factor, bilinear=bilinear)\n        self.up3 = Up(base_ch*4, base_ch*2 // factor, bilinear=bilinear)\n        self.up4 = Up(base_ch*2, base_ch, bilinear=bilinear)\n        self.seg_out = nn.Conv2d(base_ch, n_classes, kernel_size=1)\n        \n        # Classifier head on bottleneck\n        bottleneck_ch = base_ch*16 // factor\n        self.clf_pool = nn.AdaptiveAvgPool2d(1)\n        self.clf_head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(bottleneck_ch, bottleneck_ch//2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(bottleneck_ch//2, clf_classes)\n        )\n    \n    def encode(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        return x1, x2, x3, x4, x5\n    \n    def decode(self, x1, x2, x3, x4, x5):\n        # First upsampling to get the gating signal for attention\n        y = self.up1(x5, x4)  # This gives us the gating signal for next attention\n        \n        # Apply attention gates with proper gating signals\n        x3_att = self.attention2(g=y, x=x3)\n        y = self.up2(y, x3_att)\n        \n        x2_att = self.attention3(g=y, x=x2)\n        y = self.up3(y, x2_att)\n        \n        y = self.up4(y, x1)\n        logits = self.seg_out(y)\n        return logits\n    \n    def classify(self, x5):\n        pooled = self.clf_pool(x5)\n        logits = self.clf_head(pooled)\n        return logits\n    \n    def forward(self, x, return_both=True):\n        x1, x2, x3, x4, x5 = self.encode(x)\n        seg_logits = self.decode(x1, x2, x3, x4, x5) if return_both else None\n        clf_logits = self.classify(x5)\n        return seg_logits, clf_logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:39:58.961868Z","iopub.execute_input":"2025-09-11T03:39:58.962104Z","iopub.status.idle":"2025-09-11T03:39:58.975406Z","shell.execute_reply.started":"2025-09-11T03:39:58.962089Z","shell.execute_reply":"2025-09-11T03:39:58.974784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch_attention(model, loader, optimizer, seg_criterion, clf_criterion, mode=\"joint\"):\n    model.train()\n    total_loss = 0.0\n    total_seg_loss = 0.0\n    total_clf_loss = 0.0\n    total = 0\n\n    for batch in loader:\n        imgs = batch[\"image\"].to(device)\n        labels = batch[\"label\"].to(device)\n        has_mask = batch[\"has_mask\"].to(device).float()\n        masks = batch[\"mask\"].to(device)\n\n        optimizer.zero_grad()\n\n        if mode == \"seg_only\":\n            seg_logits, _ = model(imgs, return_both=True)\n            seg_loss = seg_criterion(seg_logits, masks)\n            seg_loss = (seg_loss * has_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3)).mean()\n            loss = seg_loss\n            clf_loss = torch.tensor(0.0, device=device)\n\n        elif mode == \"clf_only\":\n            _, clf_logits = model(imgs, return_both=False)\n            clf_loss = clf_criterion(clf_logits, labels)\n            loss = clf_loss\n            seg_loss = torch.tensor(0.0, device=device)\n\n        else:  # joint\n            seg_logits, clf_logits = model(imgs, return_both=True)\n            seg_loss = seg_criterion(seg_logits, masks)\n            seg_loss = (seg_loss * has_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3)).mean()\n            clf_loss = clf_criterion(clf_logits, labels)\n            loss = lambda_seg * seg_loss + lambda_clf * clf_loss\n\n        loss.backward()\n        optimizer.step()\n\n        bs = imgs.size(0)\n        total += bs\n        total_loss += loss.item() * bs\n        total_seg_loss += seg_loss.item() * bs\n        total_clf_loss += clf_loss.item() * bs\n\n    return {\n        \"loss\": total_loss / total,\n        \"seg_loss\": total_seg_loss / total,\n        \"clf_loss\": total_clf_loss / total\n    }\n\ndef run_attention_training_with_logging(model, mode=\"joint\", epochs=15, lr=1e-3, tag=None):\n    assert mode in [\"seg_only\", \"clf_only\", \"joint\"]\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n\n    best_val = float('inf')\n    best_state = None\n\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"train_seg_loss\": [],\n        \"train_clf_loss\": [],\n        \"val_loss\": [],\n        \"val_seg_loss\": [],\n        \"val_clf_loss\": [],\n        \"val_dice\": [],\n        \"val_acc\": []\n    }\n\n    time_tag = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    run_tag = tag or f\"attn_{mode}_{time_tag}\"\n\n    for epoch in range(1, epochs+1):\n        tr_metrics = train_one_epoch_attention(model, train_loader, optimizer, seg_criterion, clf_criterion, mode=mode)\n        val_metrics = evaluate(model, val_loader, seg_criterion, clf_criterion, mode=mode)\n\n        scheduler.step(val_metrics[\"loss\"])\n\n        # Save per-epoch checkpoint\n        torch.save({\n            \"epoch\": epoch,\n            \"mode\": mode,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"val_loss\": val_metrics[\"loss\"],\n        }, os.path.join(ckpt_dir, f\"{run_tag}_epoch{epoch:02d}.pth\"))\n\n        # Track best weights\n        if val_metrics[\"loss\"] < best_val:\n            best_val = val_metrics[\"loss\"]\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n            torch.save(best_state, os.path.join(ckpt_dir, f\"{run_tag}_best_weights.pth\"))\n\n        # Log history\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(tr_metrics[\"loss\"])\n        history[\"train_seg_loss\"].append(tr_metrics.get(\"seg_loss\", 0.0))\n        history[\"train_clf_loss\"].append(tr_metrics.get(\"clf_loss\", 0.0))\n        history[\"val_loss\"].append(val_metrics[\"loss\"])\n        history[\"val_seg_loss\"].append(val_metrics.get(\"seg_loss\", 0.0))\n        history[\"val_clf_loss\"].append(val_metrics.get(\"clf_loss\", 0.0))\n        history[\"val_dice\"].append(val_metrics.get(\"dice\", None))\n        history[\"val_acc\"].append(val_metrics.get(\"acc\", None))\n\n        print(f\"[Attention {mode}] Epoch {epoch:02d}: \"\n              f\"train_loss={tr_metrics['loss']:.4f} seg={tr_metrics['seg_loss']:.4f} clf={tr_metrics['clf_loss']:.4f} | \"\n              f\"val_loss={val_metrics['loss']:.4f} seg={val_metrics['seg_loss']:.4f} clf={val_metrics['clf_loss']:.4f}\")\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # Save history JSON\n    with open(os.path.join(out_dir, f\"history_{run_tag}.json\"), \"w\") as f:\n        json.dump(history, f, indent=2)\n\n    # Plot helpers (same as original)\n    def _plot_series(xs, series, title, fname, ylabel=\"Loss\"):\n        plt.figure(figsize=(7,5))\n        for lbl, ys in series.items():\n            if ys is None: \n                continue\n            plt.plot(xs, ys, label=lbl)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(ylabel)\n        plt.title(title)\n        plt.grid(True, alpha=0.3)\n        plt.legend()\n        plt.tight_layout()\n        path = os.path.join(fig_dir, fname)\n        plt.savefig(path, dpi=150)\n        plt.close()\n        return path\n\n    epochs_list = history[\"epoch\"]\n    if mode == \"seg_only\":\n        _plot_series(\n            epochs_list,\n            {\"train_seg_loss\": history[\"train_seg_loss\"], \"val_seg_loss\": history[\"val_seg_loss\"]},\n            \"Attention U-Net Segmentation Loss (seg_only)\",\n            f\"{run_tag}_seg_losses.png\"\n        )\n    elif mode == \"clf_only\":\n        _plot_series(\n            epochs_list,\n            {\"train_clf_loss\": history[\"train_clf_loss\"], \"val_clf_loss\": history[\"val_clf_loss\"]},\n            \"Attention U-Net Classification Loss (clf_only)\",\n            f\"{run_tag}_clf_losses.png\"\n        )\n    else:  # joint\n        _plot_series(\n            epochs_list,\n            {\n                \"train_seg_loss\": history[\"train_seg_loss\"],\n                \"val_seg_loss\": history[\"val_seg_loss\"],\n                \"train_clf_loss\": history[\"train_clf_loss\"],\n                \"val_clf_loss\": history[\"val_clf_loss\"]\n            },\n            \"Attention U-Net Joint Training Losses (Seg + Clf)\",\n            f\"{run_tag}_joint_losses.png\"\n        )\n        # Optional: plot total losses too\n        _plot_series(\n            epochs_list,\n            {\"train_total\": history[\"train_loss\"], \"val_total\": history[\"val_loss\"]},\n            \"Attention U-Net Total Loss (Joint)\",\n            f\"{run_tag}_total_losses.png\"\n        )\n\n    return best_val, history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:40:04.963065Z","iopub.execute_input":"2025-09-11T03:40:04.963575Z","iopub.status.idle":"2025-09-11T03:40:04.980705Z","shell.execute_reply.started":"2025-09-11T03:40:04.963552Z","shell.execute_reply":"2025-09-11T03:40:04.979990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Attention U-Net model\nprint(\"Training Attention U-Net...\")\nattn_model = AttentionUNetWithClassifier(\n    n_channels=3, \n    n_classes=seg_num_classes,\n    clf_classes=clf_num_classes, \n    bilinear=True, \n    base_ch=64\n).to(device)\n\n# Train with different modes\nprint(\"Training Attention U-Net segmentation-only...\")\nbest_val_attn_seg, hist_attn_seg = run_attention_training_with_logging(\n    attn_model, mode=\"seg_only\", epochs=10, lr=1e-3, tag=\"attn_seg_only_main\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T03:40:11.609843Z","iopub.execute_input":"2025-09-11T03:40:11.610455Z","iopub.status.idle":"2025-09-11T04:40:33.893624Z","shell.execute_reply.started":"2025-09-11T03:40:11.610434Z","shell.execute_reply":"2025-09-11T04:40:33.892889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training Attention U-Net classification-only...\")\nbest_val_attn_clf, hist_attn_clf = run_attention_training_with_logging(\n    attn_model, mode=\"clf_only\", epochs=10, lr=1e-3, tag=\"attn_clf_only_main\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T20:29:47.529314Z","iopub.execute_input":"2025-09-10T20:29:47.530194Z","iopub.status.idle":"2025-09-10T20:55:10.620365Z","shell.execute_reply.started":"2025-09-10T20:29:47.530133Z","shell.execute_reply":"2025-09-10T20:55:10.619433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training Attention U-Net joint multi-task...\")\nbest_val_attn_joint, hist_attn_joint = run_attention_training_with_logging(\n    attn_model, mode=\"joint\", epochs=15, lr=1e-3, tag=\"attn_joint_main\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T04:42:32.199059Z","iopub.execute_input":"2025-09-11T04:42:32.200128Z","iopub.status.idle":"2025-09-11T06:12:58.782386Z","shell.execute_reply.started":"2025-09-11T04:42:32.200099Z","shell.execute_reply":"2025-09-11T06:12:58.781671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate all models on validation set for detailed comparison\nprint(\"\\n=== Detailed Performance Comparison ===\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate segmentation-only performance\nseg_metrics = evaluate(model, val_loader, seg_criterion, clf_criterion, mode=\"seg_only\")\nprint(f\"Segmentation-only - Dice: {seg_metrics.get('dice', 0):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate classification-only performance\nclf_metrics = evaluate(model, val_loader, seg_criterion, clf_criterion, mode=\"clf_only\")\nprint(f\"Classification-only - Accuracy: {clf_metrics.get('acc', 0):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate joint performance\njoint_metrics = evaluate(model, val_loader, seg_criterion, clf_criterion, mode=\"joint\")\nprint(f\"Joint training - Dice: {joint_metrics.get('dice', 0):.4f}, Accuracy: {joint_metrics.get('acc', 0):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attn_metrics = evaluate(attn_model, val_loader, seg_criterion, clf_criterion, mode=\"joint\")\nprint(f\"Attention U-Net - Dice: {attn_metrics.get('dice', 0):.4f}, Accuracy: {attn_metrics.get('acc', 0):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T06:15:51.237459Z","iopub.execute_input":"2025-09-11T06:15:51.237747Z","iopub.status.idle":"2025-09-11T06:16:17.309664Z","shell.execute_reply.started":"2025-09-11T06:15:51.237725Z","shell.execute_reply":"2025-09-11T06:16:17.308608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_image(img_path, input_size=(512, 512)):\n    \"\"\"Preprocess image for model input\"\"\"\n    transform = A.Compose([\n        A.LongestMaxSize(max(input_size)),\n        A.PadIfNeeded(min_height=input_size[0], min_width=input_size[1], border_mode=0),\n        A.CenterCrop(height=input_size[0], width=input_size[1]),\n        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n        ToTensorV2()\n    ])\n    \n    img = np.array(Image.open(img_path).convert('RGB'))\n    transformed = transform(image=img)\n    return img, transformed['image'].unsqueeze(0)\n\n\ndef load_ground_truth_mask(mask_path, input_size=(512, 512)):\n    \"\"\"Load and preprocess ground truth mask\"\"\"\n    mask_pil = Image.open(mask_path)\n    if mask_pil.mode == 'RGB':\n        mask_pil = mask_pil.convert('L')\n    \n    mask_pil = mask_pil.resize(input_size, Image.NEAREST)\n    mask = np.array(mask_pil)\n    \n    # Binarize mask (assuming tumor=1, background=0)\n    mask_binary = (mask > 0).astype(np.uint8)\n    return mask_binary\n\ndef create_overlay(image, mask, color=(255, 0, 0), alpha=0.5):\n    \"\"\"Create overlay of mask on image\"\"\"\n    if isinstance(image, torch.Tensor):\n        # Denormalize if needed\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = image * std[:, None, None] + mean[:, None, None]\n        image = np.clip(image.permute(1, 2, 0).numpy() * 255, 0, 255).astype(np.uint8)\n    \n    overlay = image.copy()\n    mask_colored = np.zeros_like(image)\n    mask_colored[mask > 0] = color\n    \n    # Blend the colored mask with the original image\n    mask_indices = mask > 0\n    overlay[mask_indices] = (alpha * mask_colored[mask_indices] + \n                            (1 - alpha) * image[mask_indices]).astype(np.uint8)\n    \n    return overlay\n\ndef predict_and_visualize(model, test_csv_path, model_path, num_samples=5, device='cuda'):\n    \"\"\"Main function to predict and visualize results\"\"\"\n    \n    # Load model\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    \n    # Load test data\n    test_df = pd.read_csv(test_csv_path)\n    \n    # Filter samples that have masks for visualization\n    test_samples = test_df.dropna(subset=['mask_path']).head(num_samples)\n    \n    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n    if num_samples == 1:\n        axes = axes.reshape(1, -1)\n    \n    with torch.no_grad():\n        for idx, (_, row) in enumerate(test_samples.iterrows()):\n            img_path = row['image_path']\n            mask_path = row['mask_path']\n            \n            # Preprocess\n            orig_img, input_tensor = preprocess_image(img_path)\n            gt_mask = load_ground_truth_mask(mask_path)\n            \n            # Predict\n            input_tensor = input_tensor.to(device)\n            seg_logits, _ = model(input_tensor, return_both=True)\n            \n            # Get predicted mask\n            pred_probs = F.softmax(seg_logits, dim=1)\n            pred_mask = (pred_probs[0, 1].cpu().numpy() > 0.5).astype(np.uint8)\n            \n            # Create overlays\n            gt_overlay = create_overlay(orig_img, gt_mask, color=(255, 0, 0))  # Red for GT\n            pred_overlay = create_overlay(orig_img, pred_mask, color=(0, 255, 0))  # Green for prediction\n            \n            # Plot\n            axes[idx, 0].imshow(orig_img)\n            axes[idx, 0].set_title('Original Image')\n            axes[idx, 0].axis('off')\n            \n            axes[idx, 1].imshow(gt_mask, cmap='gray')\n            axes[idx, 1].set_title('Ground Truth Mask')\n            axes[idx, 1].axis('off')\n            \n            axes[idx, 2].imshow(gt_overlay)\n            axes[idx, 2].set_title('GT Overlay (Red)')\n            axes[idx, 2].axis('off')\n            \n            axes[idx, 3].imshow(pred_mask, cmap='gray')\n            axes[idx, 3].set_title('Predicted Mask')\n            axes[idx, 3].axis('off')\n            \n            axes[idx, 4].imshow(pred_overlay)\n            axes[idx, 4].set_title('Pred Overlay (Green)')\n            axes[idx, 4].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/segmentation_results.png', dpi=150, bbox_inches='tight')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\nimport matplotlib.pyplot as plt\n\nmodel = UNetWithClassifier(n_channels=3, n_classes=2, clf_classes=4, bilinear=True, base_ch=64)\n\n# Paths (update these to your actual paths)\ntest_csv_path = \"/kaggle/input/brisc-dataset/test_index.csv\"\n\n# You can choose which model to use:\n# For segmentation-only model:\n# model_path = \"/kaggle/input/2nd-draft-cse428-project/checkpoints/seg_only_main_best_weights.pth\"\n\n# For joint model (recommended as it's trained on both tasks):\nmodel_path = \"/kaggle/input/2nd-draft-cse428-project/checkpoints/joint_main_best_weights.pth\"\n\n# For classification-only model (won't work for segmentation):\n# model_path = \"/kaggle/working/checkpoints/clf_only_main_best_weights.pth\"\n\n# Run prediction and visualization\npredict_and_visualize(model, test_csv_path, model_path, num_samples=5, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}